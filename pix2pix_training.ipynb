{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pix2pix\n",
    "\n",
    "Adapted from https://github.com/eriklindernoren/PyTorch-GAN/tree/master/implementations/pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import resource\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from pix2pix.pix2pix_model import *\n",
    "from unet.unet.unet_parts import *\n",
    "from mlp import audio\n",
    "from mlp import normalization\n",
    "from mlp import utils as mlp\n",
    "from mlp.dataset import WAVAudioDS, PolarPreprocessing\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0 # epoch to start training from\n",
    "n_epochs = 30 # number of epochs of training\n",
    "dataset_name = 'VCTK' # name of the dataset\n",
    "batch_size = 4 # size of the batches\n",
    "lr = 0.0002 # adam: learning rate\n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "decay_epoch = 100 # epoch from which to start lr decay\n",
    "n_cpu = 4 # number of cpu threads to use during batch generation\n",
    "img_height = 64 # size of image height\n",
    "img_width = 64 # size of image width\n",
    "channels = 1 # number of image channels\n",
    "sample_interval = 100 # interval between sampling of images from generators\n",
    "checkpoint_interval = 1 # interval between model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (4096, resource.getrlimit(resource.RLIMIT_NOFILE)[1]))\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 100\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, img_height//2**4, img_width//2**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, leaky_relu=False, dropout=0.0):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(1, 64, leaky_relu=leaky_relu)\n",
    "        self.down1 = down(64, 128, leaky_relu=leaky_relu)\n",
    "        self.down2 = down(128, 256, leaky_relu=leaky_relu)\n",
    "        self.down3 = down(256, 512, leaky_relu=leaky_relu, dropout=dropout)\n",
    "        self.down4 = down(512, 512, leaky_relu=leaky_relu, dropout=dropout)\n",
    "        self.up1 = up(1024, 256, bilinear=True, leaky_relu=leaky_relu, dropout=dropout)\n",
    "        self.up2 = up(512, 128, bilinear=True, leaky_relu=leaky_relu, dropout=dropout)\n",
    "        self.up3 = up(256, 64, bilinear=True, leaky_relu=leaky_relu)\n",
    "        self.up4 = up(128, 64, bilinear=True, leaky_relu=leaky_relu)\n",
    "        self.outc = outconv(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_generator = UNet(leaky_relu=False, dropout=0.0).to(device)\n",
    "generator = UNet(leaky_relu=True, dropout=0.5).to(device)\n",
    "discriminator = Discriminator(in_channels=channels).to(device)\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    pretrained_generator = pretrained_generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_pixelwise.cuda()\n",
    "\n",
    "if cuda:\n",
    "    pretrained_generator.load_state_dict(torch.load('32_64_model_final.pt'))\n",
    "else:\n",
    "    pretrained_generator.load_state_dict(torch.load('32_64_model_final.pt', map_location='cpu'))\n",
    "    \n",
    "if epoch != 0:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load('generator_%d.pth' % (epoch)))\n",
    "    discriminator.load_state_dict(torch.load('discriminator_%d.pth' % (epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    #generator.apply(weights_init_normal)\n",
    "    if cuda:\n",
    "        generator.load_state_dict(torch.load('32_64_model_final.pt'))\n",
    "    else:\n",
    "        generator.load_state_dict(torch.load('32_64_model_final.pt', map_location='cpu'))\n",
    "    discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corpus/wav48/p292/p292_346.wav' 'corpus/wav48/p233/p233_116.wav'\n",
      " 'corpus/wav48/p313/p313_134.wav' 'corpus/wav48/p285/p285_051.wav'\n",
      " 'corpus/wav48/p227/p227_108.wav' 'corpus/wav48/p306/p306_082.wav'\n",
      " 'corpus/wav48/p272/p272_152.wav' 'corpus/wav48/p299/p299_342.wav'\n",
      " 'corpus/wav48/p317/p317_358.wav' 'corpus/wav48/p260/p260_013.wav'\n",
      " 'corpus/wav48/p260/p260_131.wav' 'corpus/wav48/p305/p305_377.wav'\n",
      " 'corpus/wav48/p237/p237_345.wav' 'corpus/wav48/p251/p251_059.wav'\n",
      " 'corpus/wav48/p241/p241_104.wav' 'corpus/wav48/p323/p323_118.wav'\n",
      " 'corpus/wav48/p376/p376_071.wav' 'corpus/wav48/p304/p304_239.wav'\n",
      " 'corpus/wav48/p280/p280_197.wav' 'corpus/wav48/p250/p250_206.wav'\n",
      " 'corpus/wav48/p243/p243_070.wav' 'corpus/wav48/p286/p286_285.wav'\n",
      " 'corpus/wav48/p343/p343_222.wav' 'corpus/wav48/p280/p280_208.wav'\n",
      " 'corpus/wav48/p360/p360_158.wav' 'corpus/wav48/p306/p306_297.wav'\n",
      " 'corpus/wav48/p274/p274_224.wav' 'corpus/wav48/p351/p351_099.wav'\n",
      " 'corpus/wav48/p273/p273_153.wav' 'corpus/wav48/p241/p241_092.wav'\n",
      " 'corpus/wav48/p306/p306_250.wav' 'corpus/wav48/p307/p307_220.wav'\n",
      " 'corpus/wav48/p231/p231_050.wav' 'corpus/wav48/p312/p312_037.wav'\n",
      " 'corpus/wav48/p240/p240_091.wav' 'corpus/wav48/p361/p361_107.wav'\n",
      " 'corpus/wav48/p336/p336_214.wav' 'corpus/wav48/p264/p264_123.wav'\n",
      " 'corpus/wav48/p294/p294_063.wav' 'corpus/wav48/p330/p330_388.wav'\n",
      " 'corpus/wav48/p259/p259_088.wav' 'corpus/wav48/p311/p311_393.wav'\n",
      " 'corpus/wav48/p258/p258_103.wav' 'corpus/wav48/p326/p326_164.wav'\n",
      " 'corpus/wav48/p314/p314_245.wav' 'corpus/wav48/p278/p278_370.wav'\n",
      " 'corpus/wav48/p281/p281_221.wav' 'corpus/wav48/p294/p294_183.wav'\n",
      " 'corpus/wav48/p284/p284_012.wav' 'corpus/wav48/p236/p236_241.wav'\n",
      " 'corpus/wav48/p285/p285_143.wav' 'corpus/wav48/p334/p334_116.wav'\n",
      " 'corpus/wav48/p264/p264_286.wav' 'corpus/wav48/p278/p278_326.wav'\n",
      " 'corpus/wav48/p260/p260_009.wav' 'corpus/wav48/p251/p251_328.wav'\n",
      " 'corpus/wav48/p297/p297_014.wav' 'corpus/wav48/p240/p240_130.wav'\n",
      " 'corpus/wav48/p336/p336_005.wav' 'corpus/wav48/p287/p287_325.wav'\n",
      " 'corpus/wav48/p288/p288_233.wav' 'corpus/wav48/p286/p286_322.wav'\n",
      " 'corpus/wav48/p313/p313_077.wav' 'corpus/wav48/p257/p257_356.wav'\n",
      " 'corpus/wav48/p267/p267_144.wav' 'corpus/wav48/p347/p347_287.wav'\n",
      " 'corpus/wav48/p226/p226_037.wav' 'corpus/wav48/p302/p302_037.wav'\n",
      " 'corpus/wav48/p306/p306_043.wav' 'corpus/wav48/p343/p343_098.wav'\n",
      " 'corpus/wav48/p258/p258_281.wav' 'corpus/wav48/p284/p284_318.wav'\n",
      " 'corpus/wav48/p285/p285_199.wav' 'corpus/wav48/p259/p259_107.wav'\n",
      " 'corpus/wav48/p236/p236_447.wav' 'corpus/wav48/p282/p282_323.wav'\n",
      " 'corpus/wav48/p362/p362_370.wav' 'corpus/wav48/p280/p280_059.wav'\n",
      " 'corpus/wav48/p258/p258_306.wav' 'corpus/wav48/p351/p351_019.wav'\n",
      " 'corpus/wav48/p361/p361_307.wav' 'corpus/wav48/p280/p280_381.wav'\n",
      " 'corpus/wav48/p259/p259_299.wav' 'corpus/wav48/p268/p268_003.wav'\n",
      " 'corpus/wav48/p233/p233_218.wav' 'corpus/wav48/p261/p261_280.wav'\n",
      " 'corpus/wav48/p318/p318_291.wav' 'corpus/wav48/p297/p297_011.wav'\n",
      " 'corpus/wav48/p243/p243_199.wav' 'corpus/wav48/p264/p264_476.wav'\n",
      " 'corpus/wav48/p263/p263_293.wav' 'corpus/wav48/p274/p274_381.wav'\n",
      " 'corpus/wav48/p268/p268_186.wav' 'corpus/wav48/p273/p273_094.wav'\n",
      " 'corpus/wav48/p253/p253_029.wav' 'corpus/wav48/p276/p276_199.wav'\n",
      " 'corpus/wav48/p284/p284_254.wav' 'corpus/wav48/p294/p294_397.wav'\n",
      " 'corpus/wav48/p273/p273_140.wav' 'corpus/wav48/p274/p274_451.wav']\n"
     ]
    }
   ],
   "source": [
    "train_files = pickle.load(open(\"train.pk\", \"rb\"))[:100]\n",
    "val_files = pickle.load(open(\"valid.pk\", \"rb\"))[:100]\n",
    "print(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4133.58it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3019.53it/s]\n"
     ]
    }
   ],
   "source": [
    "fs = 48000\n",
    "bs = batch_size\n",
    "stroke_width = 32\n",
    "patch_width = img_width\n",
    "patch_height = img_height\n",
    "nperseg = 256\n",
    "\n",
    "train_files = pickle.load(open(\"train.pk\", \"rb\"))[:100]\n",
    "val_files = pickle.load(open(\"valid.pk\", \"rb\"))[:100]\n",
    "\n",
    "stroke_mask = mlp.build_stroke_purge_mask(patch_width, patch_height, stroke_width, fs, channels=1)\n",
    "stroke_mask_not = ~stroke_mask\n",
    "\n",
    "purge_mask = stroke_mask.float()\n",
    "keep_mask = stroke_mask_not.float().to(device)\n",
    "\n",
    "preprocess = PolarPreprocessing(\n",
    "    normalization.norm_mag, \n",
    "    normalization.norm_phase, \n",
    "    patch_width,\n",
    "    patch_height,\n",
    "    include_phase=False\n",
    ")\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "with Pool(4) as p:\n",
    "    ds_valid = WAVAudioDS(files=val_files, mk_source=lambda x: x * purge_mask, preprocess=preprocess, \n",
    "                          patch_width=patch_width, proc_pool=p, nperseg=256, random_patches=False) \n",
    "    ds_train = WAVAudioDS(files=train_files, mk_source=lambda x: x * purge_mask, preprocess=preprocess, \n",
    "                          patch_width=patch_width, proc_pool=p, nperseg=256, random_patches=True) \n",
    "    \n",
    "val_dataloader = DataLoader(ds_valid, batch_size=bs, num_workers=4, shuffle=False)\n",
    "dataloader = DataLoader(ds_train, batch_size=bs, num_workers=4, shuffle=False)\n",
    "\n",
    "#with Pool(8) as p:\n",
    "#    train_dss = []\n",
    "#    \n",
    "#    for i in range(1):\n",
    "#        train_dss.append(WAVAudioDS(train_files[i*4000:(i+1)*4000], mk_source=lambda x: x * purge_mask, \n",
    "#                                    preprocess=preprocess, patch_width=patch_width, proc_pool=p, \n",
    "#                                    nperseg=256, random_patches=True))\n",
    "\n",
    "#    ds_train = MultiSet(train_dss)\n",
    "#    ds_test = WAVAudioDS(val_files, mk_source=lambda x: x * purge_mask, preprocess=preprocess, \n",
    "#                          patch_width=patch_width, proc_pool=p, nperseg=256, random_patches=False)\n",
    "\n",
    "#val_dataloader = DataLoader(ds_train, batch_size=bs, num_workers=8, shuffle=True)\n",
    "#dataloader = DataLoader(ds_test, batch_size=bs, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "def sample_images(epoch, batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    real_A = Variable(imgs[0].type(Tensor))\n",
    "    real_B = Variable(imgs[1].type(Tensor))\n",
    "\n",
    "    recon_B = generator(real_A.to(device)).detach()\n",
    "    recon_B = real_A + stroke_mask_not.float()*recon_B\n",
    "    recon_B_old = pretrained_generator(real_A.to(device)).detach()\n",
    "    recon_B_old = real_A + stroke_mask_not.float()*recon_B_old\n",
    "    \n",
    "    img_sample = torch.cat((real_A.detach(), recon_B.detach(), recon_B_old.detach(), real_B.detach()), -2)\n",
    "    torch.save(img_sample, '%s_%s.pt' % (epoch, batches_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] [Batch 2/212] [D loss: 0.169212] [G loss: 18.791370, pixel: 0.182822, adv: 0.509163] ETA: 1:56:59.46441401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/david/.conda/envs/audio_inpainting/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-55fd1c89c5f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_GAN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_pixel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_pixel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prev_time = time.time()\n",
    "results = []\n",
    "for epoch in range(epoch, n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Model inputs\n",
    "        real_A = Variable(batch[0].type(Tensor)) # Gap\n",
    "        real_B = Variable(batch[1].type(Tensor)) # Original (No gap)\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = generator(real_A)\n",
    "        pred_fake = discriminator(fake_B, real_A)\n",
    "        loss_GAN = criterion_GAN(pred_fake, valid)\n",
    "        \n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "        \n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = discriminator(real_B, real_A)\n",
    "        loss_real = criterion_GAN(pred_real, valid)\n",
    "\n",
    "        # Fake loss\n",
    "        pred_fake = discriminator(fake_B.detach(), real_A)\n",
    "        loss_fake = criterion_GAN(pred_fake, fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = n_epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s\" %\n",
    "                                                        (epoch, n_epochs,\n",
    "                                                        i, len(dataloader),\n",
    "                                                        loss_D.item(), loss_G.item(),\n",
    "                                                        loss_pixel.item(), loss_GAN.item(),\n",
    "                                                        time_left))\n",
    "        results.append((epoch, loss_D.item(), loss_G.item(), loss_pixel.item(), loss_GAN.item()))\n",
    "        with open('results.pkl', 'wb') as fp:\n",
    "            pickle.dump(results, fp)\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_images(epoch, batches_done)\n",
    "\n",
    "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(generator.state_dict(), 'generator_%d.pth' % (epoch))\n",
    "        torch.save(discriminator.state_dict(), 'discriminator_%d.pth' % (epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
