{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pix2pix\n",
    "\n",
    "Adapted from https://github.com/eriklindernoren/PyTorch-GAN/tree/master/implementations/pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import resource\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from models.pix2pix import *\n",
    "from models.unet import *\n",
    "from mlp import audio\n",
    "from mlp import normalization\n",
    "from mlp import utils as mlp\n",
    "from mlp.dataset import WAVAudioDS, PolarPreprocessing\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0 # epoch to start training from\n",
    "n_epochs = 30 # number of epochs of training\n",
    "dataset_name = 'VCTK' # name of the dataset\n",
    "batch_size = 4 # size of the batches\n",
    "lr = 0.0002 # adam: learning rate\n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "decay_epoch = 100 # epoch from which to start lr decay\n",
    "n_cpu = 4 # number of cpu threads to use during batch generation\n",
    "img_height = 64 # size of image height\n",
    "img_width = 64 # size of image width\n",
    "channels = 1 # number of image channels\n",
    "sample_interval = 100 # interval between sampling of images from generators\n",
    "checkpoint_interval = 1 # interval between model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (4096, resource.getrlimit(resource.RLIMIT_NOFILE)[1]))\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 100\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, img_height//2**4, img_width//2**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, leaky_relu=False, dropout=0.0):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(1, 64, leaky_relu=leaky_relu)\n",
    "        self.down1 = down(64, 128, leaky_relu=leaky_relu)\n",
    "        self.down2 = down(128, 256, leaky_relu=leaky_relu)\n",
    "        self.down3 = down(256, 512, leaky_relu=leaky_relu, dropout=dropout)\n",
    "        self.down4 = down(512, 512, leaky_relu=leaky_relu, dropout=dropout)\n",
    "        self.up1 = up(1024, 256, bilinear=True, leaky_relu=leaky_relu, dropout=dropout)\n",
    "        self.up2 = up(512, 128, bilinear=True, leaky_relu=leaky_relu, dropout=dropout)\n",
    "        self.up3 = up(256, 64, bilinear=True, leaky_relu=leaky_relu)\n",
    "        self.up4 = up(128, 64, bilinear=True, leaky_relu=leaky_relu)\n",
    "        self.outc = outconv(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_generator = UNet(leaky_relu=False, dropout=0.0).to(device)\n",
    "generator = UNet(leaky_relu=True, dropout=0.5).to(device)\n",
    "discriminator = Discriminator(in_channels=channels).to(device)\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    pretrained_generator = pretrained_generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_pixelwise.cuda()\n",
    "\n",
    "if cuda:\n",
    "    pretrained_generator.load_state_dict(torch.load('../files/32_64_model_final.pt'))\n",
    "else:\n",
    "    pretrained_generator.load_state_dict(torch.load('../files/32_64_model_final.pt', map_location='cpu'))\n",
    "    \n",
    "if epoch != 0:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load('generator_%d.pth' % (epoch)))\n",
    "    discriminator.load_state_dict(torch.load('discriminator_%d.pth' % (epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    #generator.apply(weights_init_normal)\n",
    "    if cuda:\n",
    "        generator.load_state_dict(torch.load('../files/32_64_model_final.pt'))\n",
    "    else:\n",
    "        generator.load_state_dict(torch.load('../files/32_64_model_final.pt', map_location='cpu'))\n",
    "    discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 632.05it/s]/home/hector/Code/MSc/MLP/mlp-final-fork/libraries/mlp/dataset.py:107: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return torch.from_numpy(freqs.reshape(129, 2, -1)[:max_freqs].transpose(1, 0, 2).astype(np.float32))\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 1136.05it/s]\n"
     ]
    }
   ],
   "source": [
    "fs = 48000\n",
    "bs = batch_size\n",
    "stroke_width = 32\n",
    "patch_width = img_width\n",
    "patch_height = img_height\n",
    "nperseg = 256\n",
    "\n",
    "train_files = pickle.load(open(\"../files/train.pk\", \"rb\"))[:1]\n",
    "val_files = pickle.load(open(\"../files/valid.pk\", \"rb\"))[:1]\n",
    "\n",
    "stroke_mask = mlp.build_stroke_purge_mask(patch_width, patch_height, stroke_width, fs, channels=1)\n",
    "stroke_mask_not = ~stroke_mask\n",
    "\n",
    "purge_mask = stroke_mask.float()\n",
    "keep_mask = stroke_mask_not.float().to(device)\n",
    "\n",
    "preprocess = PolarPreprocessing(\n",
    "    normalization.norm_mag, \n",
    "    normalization.norm_phase, \n",
    "    patch_width,\n",
    "    patch_height,\n",
    "    include_phase=False\n",
    ")\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "with Pool(1) as p:\n",
    "    ds_valid = WAVAudioDS(files=val_files, mk_source=lambda x: x * purge_mask, preprocess=preprocess, \n",
    "                          patch_width=patch_width, proc_pool=p, nperseg=256, random_patches=False) \n",
    "    ds_train = WAVAudioDS(files=train_files, mk_source=lambda x: x * purge_mask, preprocess=preprocess, \n",
    "                          patch_width=patch_width, proc_pool=p, nperseg=256, random_patches=True) \n",
    "        \n",
    "val_dataloader = DataLoader(ds_valid, batch_size=bs, num_workers=4, shuffle=False)\n",
    "dataloader = DataLoader(ds_train, batch_size=bs, num_workers=4, shuffle=False)\n",
    "\n",
    "#with Pool(8) as p:\n",
    "#    train_dss = []\n",
    "#    \n",
    "#    for i in range(1):\n",
    "#        train_dss.append(WAVAudioDS(train_files[i*4000:(i+1)*4000], mk_source=lambda x: x * purge_mask, \n",
    "#                                    preprocess=preprocess, patch_width=patch_width, proc_pool=p, \n",
    "#                                    nperseg=256, random_patches=True))\n",
    "\n",
    "#    ds_train = MultiSet(train_dss)\n",
    "#    ds_test = WAVAudioDS(val_files, mk_source=lambda x: x * purge_mask, preprocess=preprocess, \n",
    "#                          patch_width=patch_width, proc_pool=p, nperseg=256, random_patches=False)\n",
    "\n",
    "#val_dataloader = DataLoader(ds_train, batch_size=bs, num_workers=8, shuffle=True)\n",
    "#dataloader = DataLoader(ds_test, batch_size=bs, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "def sample_images(epoch, batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    real_A = Variable(imgs[0].type(Tensor))\n",
    "    real_B = Variable(imgs[1].type(Tensor))\n",
    "\n",
    "    recon_B = generator(real_A.to(device)).detach()\n",
    "    recon_B = real_A + stroke_mask_not.float()*recon_B\n",
    "    recon_B_old = pretrained_generator(real_A.to(device)).detach()\n",
    "    recon_B_old = real_A + stroke_mask_not.float()*recon_B_old\n",
    "    \n",
    "    img_sample = torch.cat((real_A.detach(), recon_B.detach(), recon_B_old.detach(), real_B.detach()), -2)\n",
    "    torch.save(img_sample, '%s_%s.pt' % (epoch, batches_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29/30] [Batch 0/1] [D loss: 0.020094] [G loss: 62.594440, pixel: 0.615239, adv: 1.070533] ETA: 0:00:00.721312"
     ]
    }
   ],
   "source": [
    "prev_time = time.time()\n",
    "results = []\n",
    "for epoch in range(epoch, n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Model inputs\n",
    "        real_A = Variable(batch[0].type(Tensor)) # Gap\n",
    "        real_B = Variable(batch[1].type(Tensor)) # Original (No gap)\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = generator(real_A)\n",
    "        pred_fake = discriminator(fake_B, real_A)\n",
    "        loss_GAN = criterion_GAN(pred_fake, valid)\n",
    "        \n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "        \n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = discriminator(real_B, real_A)\n",
    "        loss_real = criterion_GAN(pred_real, valid)\n",
    "\n",
    "        # Fake loss\n",
    "        pred_fake = discriminator(fake_B.detach(), real_A)\n",
    "        loss_fake = criterion_GAN(pred_fake, fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = n_epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s\" %\n",
    "                                                        (epoch, n_epochs,\n",
    "                                                        i, len(dataloader),\n",
    "                                                        loss_D.item(), loss_G.item(),\n",
    "                                                        loss_pixel.item(), loss_GAN.item(),\n",
    "                                                        time_left))\n",
    "        results.append((epoch, loss_D.item(), loss_G.item(), loss_pixel.item(), loss_GAN.item()))\n",
    "        with open('results.pkl', 'wb') as fp:\n",
    "            pickle.dump(results, fp)\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_images(epoch, batches_done)\n",
    "\n",
    "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(generator.state_dict(), 'generator_%d.pth' % (epoch))\n",
    "        torch.save(discriminator.state_dict(), 'discriminator_%d.pth' % (epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
